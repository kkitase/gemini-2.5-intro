{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4143bb59",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/04-model-context-protocol-mcp.ipynb)\n",
    "\n",
    "# Part 4: Model Context Protocol (MCP)\n",
    "\n",
    "The Model Context Protocol (MCP) is an open standard for connecting AI assistants to external data sources and tools. It enables seamless integration between LLMs and various services, databases, and APIs through a standardized protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dabe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "else:\n",
    "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
    "\n",
    "# Create client with api key\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f208c",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "Model Context Protocol (MCP) is a revolutionary approach to extending AI capabilities. Unlike traditional function calling where you define functions locally in your code, MCP allows AI models to connect to remote servers that provide tools and resources.\n",
    "\n",
    "\n",
    "- **üîå Plug-and-Play Integration**: Connect to any MCP-compatible service instantly\n",
    "- **üåê Remote Capabilities**: Access tools and data from anywhere on the internet\n",
    "- **üîÑ Standardized Protocol**: One protocol works with all MCP servers\n",
    "- **üîí Centralized Security**: Control access and permissions at the server level\n",
    "- **üìà Scalability**: Share resources across multiple AI applications\n",
    "- **üõ†Ô∏è Rich Ecosystem**: Growing library of MCP servers for various use case\n",
    "\n",
    "## 1. Working with Stdio MCP Servers\n",
    "\n",
    "Stdio (Standard Input/Output) servers run as local processes and communicate through pipes. This is perfect for:\n",
    "- Development and testing\n",
    "- Local tools and utilities\n",
    "- Lightweight integrations\n",
    "\n",
    "\n",
    "## 1. Working with MCP Servers\n",
    "\n",
    "Let's use the DeepWiki MCP server, which provides access to Wikipedia data and search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ecfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\"-y\", \"@philschmid/weather-mcp\"],  # MCP Server\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Prompt to get the weather for the current day in London.\n",
    "            prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n",
    "            # Initialize the connection between client and server\n",
    "            await session.initialize()\n",
    "            # Send request to the model with MCP function declarations\n",
    "            response = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],  # uses the session, will automatically call the tool\n",
    "                    # Uncomment if you **don't** want the sdk to automatically call the tool\n",
    "                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n",
    "                    #     disable=True\n",
    "                    # ),\n",
    "                ),\n",
    "            )\n",
    "            print(response.text)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ce101",
   "metadata": {},
   "source": [
    "## !! Exercise: Build Your Own MCP CLI Agent !!\n",
    "\n",
    "Create an interactive command-line interface (CLI) chat agent that connects to the DeepWiki MCP server (a remote server providing access to Wikipedia-like data). The agent should allow users to ask questions about GitHub repositories, and it will use the DeepWiki server to find answers.\n",
    "\n",
    "Task:\n",
    "- Use `mcp.client.streamable_http.streamablehttp_client` to establish a connection to the remote URL (https://mcp.deepwiki.com/mcp). \n",
    "- Inside the `async with streamablehttp_client(...)` block, create an `mcp.ClientSession`.\n",
    "- Initialize the session using `await session.initialize()`.\n",
    "- Create a `genai.types.GenerateContentConfig` with `temperature=0` and pass the `session` object in the `tools` list. This configures the chat to use the MCP server.\n",
    "- Create an asynchronous chat session using `client.aio.chats.create()`, passing the `MODEL_ID` (e.g., \"gemini-2.5-flash-preview-05-20\") and the `config` you created.\n",
    "- Implement an interactive loop to chat with the model using `input()` to get the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf73b5",
   "metadata": {},
   "source": [
    "## Recap & Next Steps\n",
    "\n",
    "**What You've Learned:**\n",
    "- Understanding the Model Context Protocol (MCP) and its advantages over traditional function calling\n",
    "- Connecting to remote MCP servers using both stdio and HTTP protocols\n",
    "- Building interactive chat agents that leverage MCP capabilities\n",
    "\n",
    "**Key Takeaways:**\n",
    "- MCP enables plug-and-play integration with external services and data sources\n",
    "- Remote capabilities provide access to tools and data from anywhere on the internet\n",
    "- Standardized protocols ensure compatibility across different AI applications\n",
    "- Centralized security and permissions improve enterprise deployment scenarios\n",
    "- The MCP ecosystem is rapidly growing with servers for various use cases\n",
    "\n",
    "üéâ **Congratulations!** You've completed the Gemini 2.5 AI Engineering Workshop\n",
    "\n",
    "**More Resources:**\n",
    "- [MCP with Gemini Documentation](https://ai.google.dev/gemini-api/docs/function-calling?example=weather#model_context_protocol_mcp)\n",
    "- [Function Calling Documentation](https://ai.google.dev/gemini-api/docs/function-calling?lang=python)\n",
    "- [MCP Official Specification](https://spec.modelcontextprotocol.io/)\n",
    "- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n",
    "- [MCP Server Directory](https://github.com/modelcontextprotocol/servers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
