{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9e5234",
   "metadata": {},
   "source": [
    "# Google Colab ではじめる Gemini API、テキスト生成やチャットを試してみよう。\n",
    "\n",
    "ここでは、Google Gen AI SDK を使用した Gemini API によるテキストの生成について、ハンズオン形式で学びます。具体的には、基本的なプロンプト、チャットの対話、ストリーミング、設定について解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf5f57",
   "metadata": {},
   "source": [
    "以下のボタンから Notebook を開いて進めましょう。\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kkitase/gemini-2.5-findy/blob/main/notebooks/01-jp-text-generation-and-chat.ipynb)\n",
    "\n",
    "以降の解説は、Google Colab で実際にコードを実行しながら進めることを想定していますが、コードと解説を読み進めるだけでも学習できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f50e48",
   "metadata": {},
   "source": [
    "[セットアップと認証](00-jp-setup-and-authentication.ipynb) のセクションを完了していることを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26641af",
   "metadata": {},
   "source": [
    "## 1. 最初のプロンプト\n",
    "\n",
    "まず、Gemini モデルと対話するためにノートブック全体で使用するクライアントオブジェクトを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Google Colab のユーザーデータから API キーを取得\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "\n",
    "# API キーを使ってクライアントを作成\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# モデル ID を設定\n",
    "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\"] {allow-input: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03bf2e",
   "metadata": {},
   "source": [
    "次に、プロンプトを作成して、テキストを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"都会に住む人をターゲットにした新しいサウナの名前を 3 つ作成してください。\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(\"Gemini からの応答:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a32ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# さまざまなプロンプトを送信してみましょう。\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab74e5c",
   "metadata": {},
   "source": [
    "## 2. トークンの理解とカウント\n",
    "\n",
    "トークンは、Gemini モデルがテキストを処理するために使用する基本的な単位です。トークンの使用状況を理解することは、次の点で重要です。\n",
    "- **コスト管理**: 請求はトークン消費量に基づいています\n",
    "- **コンテキスト制限**: モデルには最大トークン制限があります（例：Gemini 2.5 Pro の場合は 100 万トークン）\n",
    "- **パフォーマンスの最適化**: 入力が小さいほど、一般的に処理が高速になります\n",
    "\n",
    "Gemini モデルの場合、1 トークンは約英数字 4 文字に相当し、100 トークンは約 60 〜 80 の英単語に相当します。\n",
    "\n",
    "### 送信前のトークンをカウント\n",
    "\n",
    "モデルに送信する前に入力のトークンをカウントして、コストを見積もり、制限内に収まるようにすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78019bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力のトークンをカウントする\n",
    "prompt = \"あなたはサウナについて専門知識を持っています。サウナで、ととのうとはどういうことですか？\"\n",
    "\n",
    "# TODO: \n",
    "# client.models.count_tokens() メソッドを呼び出します。下記のコードを修正して、MODEL_ID と prompt を渡すようにしてください。\n",
    "# token_count = client.models.count_tokens(\n",
    "#     model=...,\n",
    "#     contents=...\n",
    "# )\n",
    "print(f\"入力トークン: {token_count.total_tokens}\")\n",
    "\n",
    "# コストの見積もり（価格例 - 現在のレートを確認してください）\n",
    "estimated_cost = token_count.total_tokens * 0.15 / 1_000_000\n",
    "print(f\"推定入力コスト: ${estimated_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58082e5b",
   "metadata": {},
   "source": [
    "### 送信後のトークンをカウント\n",
    "\n",
    "テキストを生成した後、詳細なトークン使用状況情報にアクセスできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"サウナでととのった後に飲む飲み物を 3 つ提案して。\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(f\"提案:\\n{response.text}\\n\")\n",
    "\n",
    "# トークン使用メタデータにアクセスする\n",
    "usage = response.usage_metadata\n",
    "print(f\"入力トークン: {usage.prompt_token_count}\")\n",
    "print(f\"思考トークン: {usage.thoughts_token_count}\")\n",
    "print(f\"出力トークン: {usage.candidates_token_count}\")\n",
    "\n",
    "# 合計推定コストを計算する\n",
    "total_cost = (usage.prompt_token_count * 0.15 + (usage.candidates_token_count + usage.thoughts_token_count) * 3.5) / 1_000_000\n",
    "print(f\"合計推定コスト: ${total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b3568",
   "metadata": {},
   "source": [
    "## 3. テキスト生成の基本\n",
    "\n",
    "テキストを生成する最も簡単な方法は、モデルにテキストのみのプロンプトを提供することです。 `contents` は、単一のプロンプト、リスト、またはマルチモーダル入力の組み合わせにすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab400c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_sauna_origin = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"サウナの発祥はどこですか？\"\n",
    ")\n",
    "print(f\"質問: サウナの発祥はどこですか？\\nA: {response_sauna_origin.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3783386",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_gnocchi_tokyo = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\"新しいニョッキ専門レストランの名前を考えて\", \"city: tokyo\"]\n",
    ")\n",
    "print(f\"\\n東京のニョッキ レストラン:\\n{response_gnocchi_tokyo.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# さまざまなテキストプロンプトを送信してみましょう。\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a9924",
   "metadata": {},
   "source": [
    "## 4. ストリーミング\n",
    "\n",
    "ストリーミングを使用すると、生成される応答を少しずつ受け取ることできるため、長い応答やチャットボットなどのリアルタイム アプリケーションで、より優れたユーザー体験を提供できます。\n",
    "\n",
    "**ストリーミングを使用する場合:**\n",
    "- インタラクティブなアプリケーション（チャットボットなど）\n",
    "- 長いコンテンツの生成\n",
    "- ユーザー体験の向上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa62a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_long_story = \"サウナ好きの主人公が、何らかの理由でファンタジー風の異世界へ行き、そこで新たな人生をスタートさせる物語を書いてください。\"\n",
    "\n",
    "print(\"ストリーミング応答:\")\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt_long_story\n",
    "):\n",
    "    if chunk.text:  # チャンクにテキストコンテンツがあるかどうかを確認\n",
    "        print(chunk.text, end=\"\", flush=True)\n",
    "print(\"\\n\")  # 最後に改行を追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3decfc",
   "metadata": {},
   "source": [
    "## 5. チャット（マルチターン会話）\n",
    "\n",
    "chat クラスは、会話履歴を追跡するためのインターフェースを提供します。内部的には、`generate_content` メソッドを使用しています。つまり、chat_session.send_message() を呼び出すたびに、内部では「これまでの全会話」を毎回 generate_content に送り直すという、作業が自動的に行われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7311e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = client.chats.create(model=MODEL_ID)\n",
    "\n",
    "user_message1 = \"ヨーロッパ旅行を計画しています。おすすめはありますか？\"\n",
    "print(f\"ユーザー: {user_message1}\")\n",
    "response1 = chat_session.send_message(message=user_message1)\n",
    "print(f\"モデル: {response1.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message2 = \"ニョッキが好きです。あまり高価じゃないものがいいです。\"\n",
    "print(f\"ユーザー: {user_message2}\")\n",
    "response2 = chat_session.send_message(message=user_message2)\n",
    "print(f\"モデル: {response2.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a60eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message3 = \"サウナはありますか？\"\n",
    "print(f\"ユーザー: {user_message3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8008045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# さらに会話を続けましょう\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 会話履歴を表示\n",
    "history = chat_session.get_history()\n",
    "print(f\"会話の合計メッセージ数: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13867852",
   "metadata": {},
   "source": [
    "## 6. システム インストラクション\n",
    "\n",
    "システム インストラクションを使用すると、モデルの動作と個性を定義できます。これらは会話全体で一貫して適用されます。\n",
    "\n",
    "**システム インストラクションのベスト プラクティス:**\n",
    "- 具体的に、明確に\n",
    "- 役割とトーンを定義する\n",
    "- フォーマットの好みを指定する\n",
    "- 行動ガイドラインを設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# システム インストラクションを使って、ロールやフォーマットを指定してみましょう。\n",
    "\n",
    "system_instruction_marketing_pro = \"\"\"\n",
    "# 役割\n",
    "あなたは、全く新しいフィットネスブランドを立ち上げる、マーケティングのプロです。\n",
    "\n",
    "# ガイドライン\n",
    "- 常に 3 つの異なる選択肢を提案してください。\n",
    "- 抽象的な言葉を避け、ターゲット顧客がイメージしやすい具体的な言葉を選んでください。\n",
    "- なぜその名前を提案するのか、論理的な根拠を必ず添えてください。\n",
    "\n",
    "# フォーマット\n",
    "提案は、以下の Markdown 形式を厳守してください。\n",
    "\n",
    "## 提案1: [ジムの名前]\n",
    "- **コンセプト:** (ジムのコンセプトを 1 〜 2 文で説明)\n",
    "- **ターゲット層:** (どのような顧客を対象とするか)\n",
    "- **ネーミングの由来:** (なぜこの名前にしたのか)\n",
    "\"\"\"\n",
    "\n",
    "# システム インストラクションを使ってコンテンツを生成\n",
    "response_pro = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"全く新しいフィットネスジムとその名前を考えて\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction_marketing_pro\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response_pro.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3fa23",
   "metadata": {},
   "source": [
    "## 7. Gemini モデルのパラメータ\n",
    "\n",
    "Gemini モデルのパラメータを使用して、生成の動作をカスタマイズします。これらを理解することで、応答を微調整することができます。\n",
    "\n",
    "**パラメータガイド:**\n",
    "- **Temperature (0.0-2.0)**: 回答の創造性やユニークさを調整します。例：低いと「犬」、高いと「ワンワン星人」のような答え。事実に基づいたコンテンツには 0.2 〜 0.4、創造的なコンテンツには 0.7 〜 1.0 を使用します\n",
    "- **Top-p (0.0-1.0)**: 単語候補を確率の合計値で絞り込みます。例：0.9 なら多様な表現、0.1なら定番の言葉を選びます。\n",
    "- **Top-k**: 単語候補を確率上位の個数で絞り込みます。例：30 なら候補多数、3 なら鉄板の 3 択の中から選びます。\n",
    "- **Max output tokens**: 生成される文章の最大長（文字数）です。例：2000 なら長文、10 なら一言だけの返信になります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f79eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini モデルのパラメータを使用して、生成の動作をカスタマイズしましょう.\n",
    "\n",
    "generation_config_dict = {\n",
    "    \"temperature\": 0.5,         #回答の創造性やユニークさを調整します。\n",
    "    \"max_output_tokens\": 2000,  # 生成される文章の最大長（文字数）です。\n",
    "    \"top_p\": 0.5,               # 単語候補を確率の合計値で絞り込みます。\n",
    "    \"top_k\": 20,                # 単語候補を確率上位の個数で絞り込みます。\n",
    "}\n",
    "\n",
    "response_config = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"フィットネスジムの名前を考えて\",\n",
    "    config=generation_config_dict\n",
    ")\n",
    "\n",
    "print(response_config.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ad3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Gemini モデルのパラメータを変えて、生成の動作をカスタマイズしてみましょう。\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74fed1",
   "metadata": {},
   "source": [
    "## 8. 強大なコンテキストとファイルのアップロード\n",
    "\n",
    "Gemini 2.5 には、100 万トークンという巨大なコンテキス トウィンドウを持っています。これがどれくらいの情報量かというと、具体的には下記のような例に相当します。\n",
    "\n",
    "- コード: 50,000 行のコード\n",
    "- 過去 5 年間に送信したすべてのテキストメッセージ\n",
    "- 平均的な長さの英語の小説 8 冊\n",
    "- 約 1 時間の動画データ\n",
    "\n",
    "Google Colab の File API を使用すると、ファイルを Gemini API にアップロードし、リクエストのコンテキストとして使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# サンプルテキストファイルをダウンロード\n",
    "sample_text_url = \"https://www.gutenberg.org/files/74/74-0.txt\"  # トム・ソーヤーの冒険\n",
    "response_req = requests.get(sample_text_url)\n",
    "\n",
    "# ファイルに保存\n",
    "with open(\"sample_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response_req.text)\n",
    "\n",
    "# ファイルを Gemini API にアップロード\n",
    "try:\n",
    "    myfile = client.files.upload(file=\"sample_book.txt\")\n",
    "    print(f\"ファイルが正常にアップロードされました: {myfile.name}\")\n",
    "    \n",
    "    # アップロードされたファイルをコンテキストとして使用してコンテンツを生成\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID, \n",
    "        contents=[myfile, \"この本の要点を 3 つ、箇条書きで簡潔にまとめてください。\"])\n",
    "    \n",
    "    print(\"要約:\")\n",
    "    print(response.text)\n",
    "    \n",
    "    # 大規模なコンテキストのトークン使用量を確認\n",
    "    print(f\"\\nトークン使用量: {response.usage_metadata.total_token_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ファイルのアップロード中にエラーが発生しました: {e}\")\n",
    "    print(\"ファイルが存在し、アクセス可能であることを確認してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d60a9a",
   "metadata": {},
   "source": [
    "## 9. 「本」 とチャット\n",
    "\n",
    "「不思議の国のアリス」の本と「話す」ことができるインタラクティブなチャットを作成しましょう。ペルソナを設定し、本のテキストを会話のコンテキストとして使用します。\n",
    "\n",
    "- 「不思議の国のアリス」のテキストをダウンロードします。\n",
    "- 本のテキストファイル（`alice_in_wonderland.txt`）を `client.files.upload()` を使用して Gemini API にアップロードします。\n",
    "- `client.chats.create()` を使用してチャットセッションを作成します。\n",
    "- `chat.send_message()` を使用してチャットセッションに最初のメッセージを送信します。\n",
    "- チャットセッションに少なくとも 1 つ質問を送信します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「不思議の国のアリス」の本とチャットするための準備\n",
    "import requests\n",
    "\n",
    "# 不思議の国のアリスをダウンロード\n",
    "book_text_url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "try:\n",
    "    response_book_req = requests.get(book_text_url)\n",
    "    response_book_req.raise_for_status()  \n",
    "    with open(\"alice_in_wonderland.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response_book_req.text)\n",
    "    print(\"本が正常にダウンロードされました！\")\n",
    "    \n",
    "except requests.RequestException as e:\n",
    "    print(f\"本のダウンロード中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec799247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini のモデルパラメータなどを設定して、チャットセッションを作成\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"あなたは大阪に住むユーモアあふれる書評家です。\",\n",
    "        temperature=1.2,  # 少しユニークな回答\n",
    "    )\n",
    ")\n",
    "# 保存したファイルをアップロード\n",
    "myfile = client.files.upload(file=\"alice_in_wonderland.txt\")\n",
    "# プロンプトの作成\n",
    "prompt = f\"\"\"この本の要点を 5 つ、箇条書きで簡潔にまとめてください。\n",
    "\n",
    "要約:\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message([prompt, myfile])\n",
    "print(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ec1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"X での投稿を作成してください。\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# これまでのコードを参考にして、さまざまなテキストを生成してみましょう。\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
