{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc71dad6",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/01-text-generation-and-chat.ipynb)\n",
    "\n",
    "# Part 1 - Text Generation and Chat\n",
    "\n",
    "This part focuses on text generation with the Gemini API using the `google-genai` SDK, including basic prompts, chat interactions, streaming, and configuration.\n",
    "\n",
    "Make sure you have completed the [setup and authentication](solution_00_setup_and_authentication.md) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "else:\n",
    "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
    "\n",
    "# Create client with api key\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25baada",
   "metadata": {},
   "source": [
    "## 1. Send Your First Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Create 3 names for a new coffee shop that emphasizes sustainability.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271d094",
   "metadata": {},
   "source": [
    "#### **!! Exercise: Sending Various Prompts !!**\n",
    "\n",
    "Practice sending different types of prompts to the Gemini model and observe its responses. You can also experiment with different model versions if they are available to you.\n",
    "\n",
    "Tasks:\n",
    "- Write a prompt to ask Gemini to generate a short poem about a robot.\n",
    "- Write a prompt to ask Gemini to explain \"machine learning\" in simple terms.\n",
    "- Try other models (e.g., `gemini-2.0-flash`) and send your prompts to them and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c83ca",
   "metadata": {},
   "source": [
    "## 2. Understanding and Counting Tokens\n",
    "\n",
    "Tokens are the basic units that Gemini models use to process text. Understanding token usage is crucial for:\n",
    "- **Cost management**: Billing is based on token consumption\n",
    "- **Context limits**: Models have maximum token limits (e.g., 1M tokens for Gemini 2.5 Pro)\n",
    "- **Performance optimization**: Smaller inputs generally process faster\n",
    "\n",
    "For Gemini models, a token is equivalent to about 4 characters, and 100 tokens equals about 60-80 English words.\n",
    "\n",
    "### Count tokens before generation\n",
    "\n",
    "You can count tokens in your input before sending it to the model to estimate costs and ensure you stay within limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac574ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Count tokens in the input\n",
    "# TODO: Call the client.models.count_tokens() method.\n",
    "# Make sure to pass the MODEL_ID and the prompt.\n",
    "# token_count = client.models.count_tokens(\n",
    "#     model=...,\n",
    "#     contents=...\n",
    "# )\n",
    "print(f\"Input tokens: {token_count.total_tokens}\")\n",
    "\n",
    "# Estimate cost (example pricing - check current rates)\n",
    "estimated_cost = token_count.total_tokens * 0.15 / 1_000_000\n",
    "print(f\"Estimated input cost: ${estimated_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b222b5",
   "metadata": {},
   "source": [
    "### Count tokens after generation\n",
    "\n",
    "After generating content, you can access detailed token usage information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644befed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(f\"Generated haiku:\\n{response.text}\\n\")\n",
    "\n",
    "# Access token usage metadata\n",
    "usage = response.usage_metadata\n",
    "print(f\"Input tokens: {usage.prompt_token_count}\")\n",
    "print(f\"Thought tokens: {usage.thoughts_token_count}\")\n",
    "print(f\"Output tokens: {usage.candidates_token_count}\")\n",
    "\n",
    "# Calculate total estimated cost\n",
    "total_cost = (usage.prompt_token_count * 0.15 + (usage.candidates_token_count + usage.thoughts_token_count) * 3.5) / 1_000_000\n",
    "print(f\"Total estimated cost: ${total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb35a9",
   "metadata": {},
   "source": [
    "## 3. Text Understanding with `contents`\n",
    "\n",
    "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompts, or a combination of multimodal inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_capital = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the capital of France?\"\n",
    ")\n",
    "print(f\"Q: What is the capital of France?\\nA: {response_capital.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call the client.models.generate_content() method.\n",
    "# For the contents, provide a list of strings:\n",
    "# 1. \"Create 3 names for a vegan restaurant\"\n",
    "# 2. \"city: Berlin\"\n",
    "# response_restaurant_berlin = client.models.generate_content(\n",
    "#     model=MODEL_ID,\n",
    "#     contents=[...]\n",
    "# )\n",
    "print(f\"\\nVegan restaurant names in Berlin:\\n{response_restaurant_berlin.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d9989",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "Streaming allows you to receive responses incrementally as they're generated, providing a better user experience for long responses or real-time applications like chatbots.\n",
    "\n",
    "**When to use streaming:**\n",
    "- Interactive applications (chatbots, assistants)\n",
    "- Long content generation\n",
    "- Real-time user feedback\n",
    "- Improved perceived performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_long_story = \"Write a short story about a brave knight and a friendly dragon.\"\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt_long_story\n",
    "):\n",
    "    if chunk.text:  # Check if chunk has text content\n",
    "        print(chunk.text, end=\"\", flush=True)\n",
    "print(\"\\n\")  # Add newline at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9754bc",
   "metadata": {},
   "source": [
    "## 5. Chat (Multi-turn Conversations)\n",
    "\n",
    "The SDK chat class provides an interface to keep track of conversation history. Behind the scenes it uses the same `generate_content` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = client.chats.create(model=MODEL_ID)\n",
    "\n",
    "user_message1 = \"I'm planning a weekend trip. Any suggestions for a city break in Europe?\"\n",
    "print(f\"User: {user_message1}\")\n",
    "response1 = chat_session.send_message(message=user_message1)\n",
    "print(f\"Model: {response1.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f88e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message2 = \"I like history and good food. Not too expensive.\"\n",
    "print(f\"User: {user_message2}\")\n",
    "# TODO: Call the chat_session.send_message() method with user_message2.\n",
    "# response2 = chat_session.send_message(message=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View conversation history\n",
    "history = chat_session.get_history()\n",
    "print(f\"Total messages in conversation: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0be978",
   "metadata": {},
   "source": [
    "## 6. System Instructions\n",
    "\n",
    "System instructions let you define the model's behavior and personality. They're applied consistently throughout the conversation.\n",
    "\n",
    "**Best practices for system instructions:**\n",
    "- Be specific and clear\n",
    "- Define the role and tone\n",
    "- Include formatting preferences\n",
    "- Set behavioral guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23844462",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_poet = \"You are a renowned poet from the 17th century, specializing in sonnets. Respond in iambic pentameter and use eloquent, period-appropriate language.\"\n",
    "\n",
    "response_poet = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What are your thoughts on modern technology?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction_poet\n",
    "    )\n",
    ")\n",
    "print(f\"\\nPoet model on modern tech:\\n{response_poet.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09954e6a",
   "metadata": {},
   "source": [
    "## 7. Generation Configuration\n",
    "\n",
    "Customize the generation behavior using configuration parameters. Understanding these helps you fine-tune responses for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration using dictionary\n",
    "generation_config_dict = {\n",
    "    \"temperature\": 0.2,      # Lower = more deterministic, higher = more creative\n",
    "    \"max_output_tokens\": 2000, # Limit response length\n",
    "    \"top_p\": 0.8,            # Nucleus sampling - diversity of token selection\n",
    "    \"top_k\": 30,             # Consider top 30 most likely tokens\n",
    "\n",
    "}\n",
    "\n",
    "# TODO: Call client.models.generate_content()\n",
    "# Pass the MODEL_ID, a prompt to \"Write a very short tagline for a new brand of eco-friendly sneakers.\",\n",
    "# and the generation_config_dict.\n",
    "# response_config = client.models.generate_content(\n",
    "#     model=...,\n",
    "#     contents=...,\n",
    "#     config=...\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d91651",
   "metadata": {},
   "source": [
    "**Parameter Guide:**\n",
    "- **Temperature (0.0-2.0)**: Controls randomness. Use 0.2-0.4 for factual content, 0.7-1.0 for creative content\n",
    "- **Top-p (0.0-1.0)**: Controls diversity. Lower values = more focused, higher = more diverse\n",
    "- **Top-k**: Limits token choices. Lower = more focused, higher = more diverse\n",
    "- **Max output tokens**: Prevents overly long responses and controls costs\n",
    "\n",
    "## 8. Long Context and File Uploads\n",
    "\n",
    "Gemini 2.5 Pro has a 1M token context window. In practice, 1 million tokens could look like:\n",
    "\n",
    "- 50,000 lines of code (with the standard 80 characters per line)\n",
    "- All the text messages you have sent in the last 5 years\n",
    "- 8 average length English novels\n",
    "- 1 hour of video data\n",
    "\n",
    "The File API allows you to upload files to the Gemini API and use them as context for your requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0964dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a text file (more reliable than the audio example)\n",
    "import requests\n",
    "\n",
    "# Download a sample text file\n",
    "sample_text_url = \"https://www.gutenberg.org/files/74/74-0.txt\"  # Adventures of Tom Sawyer\n",
    "response_req = requests.get(sample_text_url)\n",
    "\n",
    "# Save to local file\n",
    "with open(\"sample_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response_req.text)\n",
    "\n",
    "# Upload the file to the Gemini API\n",
    "try:\n",
    "    myfile = client.files.upload(file=\"sample_book.txt\")\n",
    "    print(f\"File uploaded successfully: {myfile.name}\")\n",
    "    \n",
    "    # Generate content using the uploaded file as context\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID, \n",
    "        contents=[myfile, \"Summarize this book in 3 key points\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Summary:\")\n",
    "    print(response.text)\n",
    "    \n",
    "    # Check token usage for the large context\n",
    "    print(f\"\\nToken usage: {response.usage_metadata.total_token_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n",
    "    print(\"Make sure the file exists and is accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465f36e",
   "metadata": {},
   "source": [
    "## 9. !! Exercise: Chat with a \"Book\" !!\n",
    "\n",
    "Create an interactive chat session where you can \"talk\" to the book \"Alice in Wonderland\". You'll set up the chat with a specific persona for the AI and use the book's text as context for the conversation.\n",
    "\n",
    "Task: \n",
    "- Download the text of \"Alice in Wonderland\" (a helper code block is provided).\n",
    "- Upload the book's text file (`alice_in_wonderland.txt`) to the Gemini API using `client.files.upload()`.\n",
    "- Create a chat session using `client.chats.create()`:\n",
    "- Send an initial message to the chat session using `chat.send_message()`:\n",
    "- Send at least one follow-up question to the chat session (e.g., \"Explain the various methods of speech delivery in more detail\") and print its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22740e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download Alice in Wonderland\n",
    "book_text_url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "try:\n",
    "    response_book_req = requests.get(book_text_url)\n",
    "    response_book_req.raise_for_status()  # Raise an exception for bad status codes\n",
    "    \n",
    "    with open(\"alice_in_wonderland.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response_book_req.text)\n",
    "    print(\"Book downloaded successfully!\")\n",
    "    \n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error downloading book: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76159ebc",
   "metadata": {},
   "source": [
    "## Recap & Next Steps\n",
    "\n",
    "**What You've Learned:**\n",
    "- Basic text generation with `client.models.generate_content()` for single prompts\n",
    "- Token counting and cost estimation for better resource management\n",
    "- Streaming responses with `generate_content_stream()` for improved user experience\n",
    "- Multi-turn conversations using `client.chats.create()` and chat sessions\n",
    "- System instructions for consistent model behavior and personality\n",
    "- Generation configuration parameters for fine-tuning responses\n",
    "- Long context handling and file uploads with the File API\n",
    "- Error handling and best practices for production applications\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Monitor token usage to control costs and stay within limits\n",
    "- Use streaming for interactive applications and long responses\n",
    "- Configure parameters based on your use case (factual vs creative content)\n",
    "- Implement proper error handling for robust applications\n",
    "- System instructions are powerful for setting behavior and tone\n",
    "\n",
    "**Next Steps:** Continue with [Part 2: Multimodal Capabilities](https://github.com/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb)\n",
    "\n",
    "**More Resources:**\n",
    "- [Text Generation Guide](https://ai.google.dev/gemini-api/docs/text-generation)\n",
    "- [Token Counting Guide](https://ai.google.dev/gemini-api/docs/tokens)\n",
    "- [Long Context Documentation](https://ai.google.dev/gemini-api/docs/long-context)\n",
    "- [File API Documentation](https://ai.google.dev/gemini-api/docs/files)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
